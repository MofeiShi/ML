{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Ass1v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MofeiShi/ML/blob/master/Ass1v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozsVZ1nvwMsN",
        "colab_type": "text"
      },
      "source": [
        "# Draft and Experiment Area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NICmlt-bwMsP",
        "colab_type": "text"
      },
      "source": [
        "1. First impression\n",
        "    * What is my chosen paper to read?\n",
        "      \n",
        "    Generative Adversarial Nets\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    * What type of the main contribution the paper has made?\n",
        "        - A theory or proposition (revealing something, from unknown to known)\n",
        "        - A method or algorithm (inventing a technique, from undoable to doable)\n",
        "        \n",
        "    The paper contributed a technique that provides a way to mean to bypass one of the challenges in designing generative models.  \n",
        "    * _Before_ reading the main body of the paper, write down your first impression  obtained from its abstract and short introduction.\n",
        "    \n",
        "    My first impression is this is an article about the generative model. From what I have read from the abstract, I sensed this paper was describing a new technique. It included the background of the research area, the challenges of the other approaches, the theoretical support of the current technique, and analysis of a test that compares different approaches.  \n",
        "    \n",
        "    * Why does the paper attract you, such as, How it surprised you? Why do you think it addresses an important topic that will be helpful in your future study of machine learning?\n",
        "    \n",
        "    The idea that two model work against each other attracted me. I was interested in this idea because I heard the artificial intelligence ---- AlphaGo which won against the world best human player ---- Ke Jie in Go, was trained by play against a copy of itself.  I am curious about the detailed mechanism of training two competing networks. I believe this technique can enlighten me in my future design of not only generative networks but neural networks in general. \n",
        "    \n",
        "2. Read the paper abstract and introduction, list here all the notions that you don't know the precise meaning. If you think you have completed your list,  compare the list with people around you who have chosen the same or a similar paper.\n",
        "\n",
        "  Generative models, an adversarial process, the data distribution, a discriminative model, a minimax two-player game, recovering the training data distribution, multilayer perceptrons, backpropagation, Markov chains, Unrolled Approximate inference networks.\n",
        "\n",
        "3. (During the next 7 days) Re-consider the central problem of the paper.\n",
        "\n",
        "  The central problem of the article is using an adversarial process to train a generative model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUbSYc5hwMsQ",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"Generative Adversarial Nets\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQtcfP4KwMsQ",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "Creation was once thought a defining gap between human intelligence and artificial intelligence, but recently, with the advance of generative modelling, this gap is gradually merging. Generative model can help engineer and mathematician to study high dimensional probability distributions. It can also be used to generate simulation for reinforcement learning, or problem solving. And lastly, generative models may generate outputs that are highly similar to the training samples. These outputs can be used to train artificial intelligence. \n",
        "The inner working of generative modelling is to train neural network to generate representation of the training samples. One of the directions of generative modelling is to learn from many training samples from a distribution, then generate outputs that bear resemblance of the same distribution. The current article will review and report on the Generative Adversarial Networks (GAN) (Goodfellow, et al., 2014) framework. The original paper covered the following sections: \n",
        "1.\tThe background of the generative modelling \n",
        "2.\tThe related generative modelling approaches\n",
        "3.\tThe mechanism of GAN\n",
        "4.\tThe theoretical results of the:\n",
        "a.\tGlobal Optimality\n",
        "b.\tConvergence of the algorithm\n",
        "5.\tThe experiments that compare GAN with the other generative models\n",
        "6.\tThe discussion of the advantages and disadvantages of GAN\n",
        "7.\tConclusions and future work\n",
        "\n",
        "The current review report consists:\n",
        "1.\tThe description of the main content of GAN\n",
        "2.\tThe innovation of GAN compares with the other approaches at that time\n",
        "3.\tThe discussion of the technical quality of the original paper\n",
        "4.\tThe applications and the influences of the original paper\n",
        "5.\tThe discussion about the presentation of the original paper       \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYJnjP7uwMsR",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzhdTlVAwMsR",
        "colab_type": "text"
      },
      "source": [
        "The GAN framework proposed two networks: a generator network (D) which generates images, and a discriminator network (G) that distinguish between real and fake images. For the purpose of generative modelling, the generator network is the primary training target, and the discriminator serves nothing after the training process. The training process involves a two-player minimax game. Because the goal is trying to train a generative network that can adequately represent training distribution, the generator network should try to fool the discriminator network into determining the fake images generated by the generator network to be real. The generator works by inputting a random noise z, and apply generation processes learned from training to create fake images.  \n",
        "<img src='https://drive.google.com/uc?id=1jx-lFNeAp3m8Jg4lm91lqkaRnWWYDZB4' alt='Conv'>\n",
        "\n",
        "The Minimax objective function is as follow: \n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Cuv18qeSOOPIK7q755DcDLfenvGwMv8T' alt='Conv'>\n",
        "\n",
        " \n",
        "The current article will refer to this function as Function 1. The first term of the right hand of the Function 1 represents the expected value of the discriminator network when it pulls samples from the data population x. D(x) should try to reach the maximum since the images from population x are real. On the other hand, the second term of the right hand of the Function 1 represents the expected value when the samples come from the image population z that was generated by the generator network. It should try to reach the minimum to undermine the objective. \n",
        "During the development, it was found that Function 1 has difficulty in training the expected value when the samples come from the image population z on the early stage. The reality was, when the generator was underdeveloped, the discriminator can easily rule the fake images as fake, the generator gained little information on how to make create images that looked real. A resolution was devised, rather than trying to minimize the expected value when the samples come from the image population z, the model should train on and try to maximize D(G(z)). That is, the generator should be trained how to make images that maximize the discriminator’s mistake. The difference was that the old generator tried to learn from its own successes, the new generator tried to learn from the discriminator’s mistake. The experiment showed the trying to maximize D(G(z)). \n",
        "\n",
        "During the training process, the model alternate between gradient ascent on the discriminator network and gradient ascent on the generator network. In practice, k steps of training the discriminator network per one step of training the generator network. This setting aims to ensure the discriminator network can have advantage on the generator network. The ultimate goal is having the discriminator network have only 50% of accuracy.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Qv9D-fwMsS",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgLfQmSpwMsS",
        "colab_type": "text"
      },
      "source": [
        "Generative modelling was not a small research area, many approaches were formulated with different prerequisites and assumptions. Due to the limitation of the length of the current article, only models that were close to GAN will be discussed. Goodfellow (2016) restricted the scope of the research area to related works that attempted to maximizing the likelihood.  \n",
        "\n",
        "<img src='https://drive.google.com/uc?id=12k4dnpOv7dhq-A_9roGKMUDtWG8GP4-z' alt='Conv'>\n",
        "\n",
        "\n",
        "Some appraches described by the original paper were as follows: Boltzmann machines were undirected garphical models with latent variables; Deep belief networks combined a single undirected layer and several directed layers; Generative stochastic network could be seen as defining a parameterized which is train the parameters of the network which performed one step of Markov chain for sampling. Also, in the original paper (Goodfellow, et al., 2014), four generative approaches were compared and summarized. \n",
        "\n",
        "<img src='https://drive.google.com/uc?id=106VUrwhJOiQyJO854u17HlX0A12knuNl' alt='Conv'>\n",
        "\n",
        " \n",
        "\n",
        "A central point that was repeatedly emphasize in the paper (Goodfellow, et al., 2014) was that the GAN framework was designed to avoided the drawback of Markov Chain. Goodfellow (2016) defined the Markov chain approximations as a process for generating samples by repeatedly drawing a sample x’ ~ q (x’|x). By repeatedly updating x according to the transition operator q, Markov chain methods can sometimes guarantee that x will eventually converge to a sample pmodel(x). But this convergence causes the speed of training to slow down significantly. This method is no longer suitable to the current environment where ImageNet can provide high dimensional images. The efficiency of Markov Chain methods is a serious problem that should be addressed. GAN does not have the Markov Chain incorporated into its process; therefore, it is bypassing the drawback Markov Chain inherited.\n",
        "Last but not least, the concept that have a framework that has two of its subordinated networks working against each other was quite innovating.     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIt3Y21hwMsT",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVhoru1BwMsT",
        "colab_type": "text"
      },
      "source": [
        "The technical quality of the original paper (Goodfellow, 2014) was high. The authors composed the idea of the GAN framework, then, he devised a mathematical model and a section of pseudo code to represents the theoretical process. The GAN framework was given explanatory description. The authors also included the developing process of the training process. Specifically, the generator network was first designed to learn to generate realistic images. They discovered that such design did not work well. They improve the model by changing the generator’s learning objective. And the setback was solved. This incident showed the authors develop the model from both the theoretical and the empirical angle.  \n",
        "\n",
        "The authors also conceptualized the theoretical results of the model, which includes the global optimality and the convergence of algorithm. They also devised proof of the theorem related to the GAN model, and made proposition. These showed high level of competency.     \n",
        "\n",
        "Lastly, several experiments were conducted to compare the performance of different generative networks. It seems counter-intuitive that the method section was not separated with the result section. However, the authors recorded specific values and the settings of the experiments was comprehensive. It would convenient for the future researchers to replicate the experiments. The sample of the results shown in the original paper was representative, and they were sufficiently explained. \n",
        "Overall, the original paper has high technical quality.  \n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC5HJbuZwMsU",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLM-G-2-wMsU",
        "colab_type": "text"
      },
      "source": [
        "The original paper did not specifically mention the application of GAN, but later at 2016, Goodfellow introduced several applications of GANs. It is noteworthy that the original paper proposed a Generative Modelling framework, based on the idea, other researchers developed something else. Zhu et al. (2016) designed interactive generative adversarial networks (iGAN). This network can refine the drawing of the user, making rough sketch looks more realistic. More interestingly, Radford et al,  (2016) showed, via interpretable vector math, GAN can be used to generate images that it was not trained before. For example, use average Z vectors and arithmetic process, samples from the smiling women group minus samples from the Neutral women group plus sample from the neutral man group would result in GAN generating images of smiling man. \n",
        "\n",
        "By the end of 2018, there were 510 papers named GAN, and when it was 2014, there was only 1. (https://github.com/hindupuravinash/the-gan-zoo) This shows the popularity of GAN. \n",
        "\n",
        "But more importantly, the original paper opened people’s mind about how neural networks can be trained. It built the fundamentals of how to set two neural networks to compete against each other. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX_1RMjgwMsV",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUhb2uFpwMsV",
        "colab_type": "text"
      },
      "source": [
        "It was very difficult for me to read. Admittedly, my ignorance in this research area could be the source of the reading difficulty. However, I do think there exist many problems in the presentation of the original paper. Firstly, many terms were not explained. For example, a central point of the paper ---- Markov Chain, was not directly defined. However, in the author’s later work (Goodfellow, 2016), it was very comprehensive, many terms were discussed comprehensively. Secondly, some concepts can be explained better with the aid of visualization. Compare to Yeung (2017), the use of visualization was less impressive. Lastly, the experiments section was mixed with the both the experiments settings and the results, making reading difficult.       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgbBp4PmwMsW",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "Goodfellow, I., 2016. NIPS 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160.\n",
        "\n",
        "Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, Sherjil., Courville, Aaron. & Bengio, Y. 2014.\n",
        "Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).\n",
        "\n",
        "Li, F., Johnson, J. & Yeung, S. 2017, ‘Lecture 13: Generative Models’. viewed 26 August 2019, \n",
        "<http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf>.\n",
        "\n",
        "Stanford University School of Engineering 2017, Lecture 13 | Generative Models, videorecording, YouTube, viewed 26 August 2019, < https://www.youtube.com/watch?v=5WoItGTWV54>.\n",
        "\n",
        "Steven Van Vaerenbergh 2018, Ian Goodfellow: Generative Adversarial Networks (NIPS 2016 tutorial), videorecording, YouTube, viewed 26 August 2019, <https://www.youtube.com/watch?v=HGYYEUSm-0Q>.\n",
        "\n",
        "[1]:https://google.com"
      ]
    }
  ]
}